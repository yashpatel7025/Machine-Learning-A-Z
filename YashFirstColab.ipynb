{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YashFirstColab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEq3z66pXMYy",
        "colab_type": "code",
        "outputId": "f03ff7dd-22ab-451b-9a4c-6680d9cd3dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#code resource--->https://github.com/sudharsan13296/Hands-On-Meta-Learning-With-Python/blob/master/02.%20Face%20and%20Audio%20Recognition%20using%20Siamese%20Networks/2.4%20Face%20Recognition%20Using%20Siamese%20Network.ipynb\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpvpVQ6Iai7R",
        "colab_type": "code",
        "outputId": "18101344-7525-4a11-f2d6-90f9fdd55ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=9a22d97a7aeec97a640fb00dc1a30698d3de8b657ec667d10e8ac0a4620d8d40\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.4 GB  | Proc size: 2.2 GB\n",
            "GPU RAM Free: 15473MB | Used: 807MB | Util   5% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJd36vD733X6",
        "colab_type": "code",
        "outputId": "b9dd66cd-3809-4ea0-8d69-be92a5b01c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/yashpatel7025/Machine-Learning-A-Z.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Machine-Learning-A-Z'...\n",
            "remote: Enumerating objects: 445, done.\u001b[K\n",
            "remote: Counting objects: 100% (445/445), done.\u001b[K\n",
            "remote: Compressing objects: 100% (444/444), done.\u001b[K\n",
            "remote: Total 10851 (delta 0), reused 445 (delta 0), pack-reused 10406\u001b[K\n",
            "Receiving objects: 100% (10851/10851), 225.59 MiB | 28.32 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n",
            "Checking out files: 100% (10407/10407), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzNwgQD3Dvg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "# Installing Keras\n",
        "# Enter the following command in a terminal (or anaconda prompt for Windows users): conda install -c conda-forge keras\n",
        "\n",
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7b3Dp0RXEmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the CNN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
        "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "classifier.add(Flatten())\n",
        "\n",
        "# Step 4 - Full connection\n",
        "classifier.add(Dense(units = 128, activation = 'relu'))\n",
        "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the CNN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8PT36zpXWjx",
        "colab_type": "code",
        "outputId": "4aa9a772-5931-43bc-977e-a80ff1048143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/Machine-Learning-A-Z/dataset',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('/content/Machine-Learning-A-Z/dataset',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 images belonging to 2 classes.\n",
            "Found 10000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FqfyXpoXcjj",
        "colab_type": "code",
        "outputId": "8ee5a838-073d-4e3e-b4a1-7180128437b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "classifier.fit_generator(training_set,\n",
        "                         steps_per_epoch = 8000,\n",
        "                         epochs = 25,\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 2000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7b5e0f11aa65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m classifier.fit_generator(training_set,\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          validation_steps = 2000)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePHuiPYiUITU",
        "colab_type": "code",
        "outputId": "1ab161eb-ebca-4924-a14a-70d3c1e78226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/yashpatel7025/Machine-Learning-A-Z.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Machine-Learning-A-Z'...\n",
            "remote: Enumerating objects: 445, done.\u001b[K\n",
            "remote: Counting objects: 100% (445/445), done.\u001b[K\n",
            "remote: Compressing objects: 100% (444/444), done.\u001b[K\n",
            "remote: Total 10851 (delta 0), reused 445 (delta 0), pack-reused 10406\u001b[K\n",
            "Receiving objects: 100% (10851/10851), 225.59 MiB | 32.91 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n",
            "Checking out files: 100% (10407/10407), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3xn7fwwyf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/content/Machine-Learning-A-Z/dataset/'\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Input, Lambda, Dense, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejqfSQ7Ow_wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_image(filename, byteorder='>'):\n",
        "    \n",
        "    #first we read the image, as a raw file to the buffer\n",
        "    with open(filename, 'rb') as f:\n",
        "        buffer = f.read()\n",
        "    \n",
        "    #using regex, we extract the header, width, height and maxval of the image\n",
        "    header, width, height, maxval = re.search(\n",
        "        b\"(^P5\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n])*\"\n",
        "        b\"(\\d+)\\s(?:\\s*#.*[\\r\\n]\\s)*)\", buffer).groups()\n",
        "    \n",
        "    #then we convert the image to numpy array using np.frombuffer which interprets buffer as one dimensional array\n",
        "    return np.frombuffer(buffer,\n",
        "                            dtype='u1' if int(maxval) < 256 else byteorder+'u2',\n",
        "                            count=int(width)*int(height),\n",
        "                            offset=len(header)\n",
        "                            ).reshape((int(height), int(width)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS_Di5GJxGZi",
        "colab_type": "code",
        "outputId": "be09bfb2-bd84-4c2c-8c30-5ccf1b5ccc36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "Image.open(dataset_path+'s1/1.pgm')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFwAAABwCAAAAAC26kjJAAAZDUlEQVR4nC2ZS6+u2XWV55hzrrXe\n97vtvc+tLq4qm9hxiBEoISQEQgOJQBo06CAhevkttGgh0YEOiN8AEihEkIQEEWECWCE2DvhSKdtV\ndU6d297f7X3Xmhca5heM1pCe8Qz8Aox5ei1b03Z7M9n9uJ5Wxiptmg9PP/CXb4/vn+6fx/ZJ79d6\nffbxMzd/96Xo9aq4DJlY+Xqedgc6ye3z/fmuvzoN3qcVCvV2ae1EZSy5ffsevBwXqMtmu626L5++\nc5ftJ/uDvb5ZirZyvsyYxqVkjGOss6Sd6eAFl82QyzTuih0j4fFmxxZQCimDAPM8tpdy3Q9zvtkW\nrjdNt88+LbopR2rbV9tnb6a+59rLviyQt9xtaJ9LaycjRLFnNHu10/BCfQny4qpWTJjd2h2dbn1e\nbzdvl25N3zy8f/PD65OXlxf7c3IdPPZeKOuQ0SiFB+/gysADAoU2ztXJZh7zfQ9+vSOGOhHCkU/6\n+fG2g3q89IcnN2PctPvr/MXzP/eaP50XRmMq8zFnmsXqsh0kU5uKTpfnZwnUrBeDDxy3TO2hLTJE\nhiJY1mudbOpsqW9O7YSbybnNKs99z3/6lZf70+64jSpbecOHzcPNsTEHt/lxAmXb7i8PU+Q57Yu7\nq+Zbva66SIKjcEjkmcGau8tyvV7jlT3+8uF6jMv6RV7O0j99XGjZ0U25ma/TYc8TsPPoPD+tMyFO\nbVPvdiyLuAZ9cUZWisa8Cg9loLTrXZQhcZ+bUfSDzf16mR4PkUHRyv07GwqO+Yhq0pzm610lb0mX\nx1vxxa90Ez3Po/pYVkVeLzqnXcupgJHC/mTqb8Z69fUSIfX2ILfvC93fbbbbV3fb+0Z7eeR366Xt\nd7EzrtfN4/3jlsuDS+V5JgPUrg/dUswv26z5bJulk5aVQ+vb/e6VdaEO2U7juOE4Ubt8+QjYvp8e\nHWsqF683INrLlsbQaywPm3kQX0rheyP3eT2P8/Yal4Pl8e54LdDkpDhvPvpuvW/3WWXj97yk7dsO\n6Y9mWmx7ikdR5k3HlLkbdbOJa2+yvZil7xf2KbZFO6nvrmW9oi3seS80igbK1RX/k7ZFWlD4+zdn\n5dsqbTuNrNTkvGmmvNEaLkSe+5bjtGoc6Ox+nC+5ag1AIRlNl2vUsn/ok7dFpet1jmXH113fHyee\nNqq6PdSZtbSSiJSQXovs5ljA7IK7B9zkfB2j9s2lS6wTQqJnrrt7v3GdXh2evPbsqslgDXv2hoz0\nse2nw6XctY2qZkqwJkdyKU32HMJOlSTrWiZt5z5KtHHxGmX/kOvukg+86qH59jIVzjGrjCKE+lpb\ngKe6eRT8jjQRytAkK9JFc9YdGi+bNi6k1NoxbXKaz9dSYzzf+KCy3j70wUdlOzzoGqfZJfTajmXz\nIMd3b47nbLvdvD6ZY7aQ0BTSdJHCcWjYr7xN7rxByUPmtbRe20PIewstfm4VC29fYdidjpI9Xu0i\nVUTeycOP84vFxy33jtvqJV3ESwiSE1uOumnq2or3itTKFLHZ+et1S9eId1/0scfm5Y2+feSv7/C2\n1tf3bEykvLfLF1/fPdSuemy620KoUNXCLEkk0Clo1uhZd7UXBKIXZ0XElmwu64hHp6Xv7eZodtw9\nWhE9A4NIU3M9iXh9cmp2ePx4s0cBt2RpEjWYmFhDqjEjxZBGgCddt6cSul10yJXy3dZyAtaNUdrm\n04OUS10TrPq2DsNoe+rbxzfblgEuhCZCDGIVIolMwPgs1MWWIq62xJXSqRAx+mHjTm5jb/Lk26/q\nw+2tvtlykFLNiNd9quNpD92QhkSJSaIEI70xktJdoUaZmZ7uWH2VJM5g1TGV48yxXm7yuLHPvvS9\nNZ9v2Kpm6nT1VV5tr0JQKFUAjVRJQrMEHIHqhM6IGkFGRBkokUjikrJW221yjZonwlj4tLvnB9uU\nSgENXC1TeRq6KeLJxVmQSgJOBjs4AwyRwZxuFICzM1wiJL2kplAddt2N+80StWrBaZMuGjyNlr7t\n3mNL2SuDKjMzKTGxJwvCKRluNEY6RNKRlChOFMMiDBkBHKBNcmo81VnOxaUTL9a91S2tuyFck1NC\nIEHErFmJRDg5s0phbaRM4gmwu0kwiSQ82JNGKm1RfJUsUi+KTHDatd2aYD/PTcApqcKkhRDJKJ7M\nLEquIZwE4VQFIBSdQkAcklwiEknnHeeyttPwd+CLBDMUD4OnVRxIC63KBCQzEglCKkh5FmKwIDwy\nncKIBBHGBGhXCoYOPbfxrHF2n+beIvg0bXnht9gcO2WwUBLUiYUyANek8AgaHplMCQskJ4E0kxyR\nAMsApUULuZTr0yfN5bOYVwEz3fUSj6ljlkxiFWEpgshgBidBCznCg+CJSEGyA+FK7MlKlJkAi4ma\nX6xfx136A4RJhz7QjWsNDG5BQoJASVDXHEgOTypckxHJiCxJQQB7MhEjmDgkktySwWVzste+bF9H\nyGBecGoF63VbJqlbVUTRRIQjFVw4keqUSglABAwqQQkFMVO2YICUU5LXUja5vV6zL6UIE7GWgbMt\nQJXNXW1oFKAAkZOLIKUINdZkFXVLIqEi7E4AlNUEkfAhWcZE1/TzmmHJGBDwPqwIdFjXWEklW4iS\nRNRIH0tmgHgKUHdmZUSKEVgDHk7MyYoEm+btmi13E5MGiCai4FIej9O00VZFqwo4yJ0I6lWYm0PA\nCAF2hadkkXTOsEzWQhouCCkGdrG7oHJZ7jZLhTF5KjfrYcubVjcysbAQhBgQ0gSBKoQJwkk2wcuq\nU8AgNTncEWBypURwcl12qXx8xXmIiUloMC9m03XDq2CujTORHKCUpCAKlEzOLHNBJKNRiOcgB4UY\ncSo4kIUhUdGg7d1q/JBkSgAvYvW9PJ6WrCREHpFOEICZ4MgkZh6RNFFejSgBwJJSUjIpnOGURAHa\ndfT7D+MRW3ZbAsm2Pz/emdcJtN9rGoU7hWeyFAaCksmEBmA6MUVYgszIR2YSA541OZlo6FTW3Y/8\nuOFgATz5TNObZbT20L3oSB5kGY7ISCSSHZykgzPABEktNMjp6mbRM4yAVINJwHZ83h/HZSkTdkHE\njKWdjhtvNG53jJHDWZgHEboFCEzmIPjoNJwCfSQ50n+a3gMaJAKkOEmdlhWr9FpjMhUVl/Xl1pdu\nLB5ZksRCNJMpMwWUSbFosgeFe1ImqZuEkFOEp8taSI0CBFQJaFQXz4hUYcv57s1poWlQejbEsUwk\nphhqKVbYKMmZeESNBIiIJdXCxSJhYmujeUTCY//pUrb3twurJ6BW54e72c6bqyYZg3M93oYtbaEh\n8DF5BvfGJpbpAQ0ydVt7XktKNWlXHsbCbBK7H13jft9y6lRJTNl98+qD8/FuIacQDb+IL9pPPjL0\nZrUdyaB1IitZQtDFeFxen1fibOtOclP9QgipPNph/2cr6N3PtZnpKsq+OQ7qUYidA9mpCFn2U1hO\n+XYzVqGkKVLAZgVYO4/VyK9Zoket3pl7502HxDRR5C4Py+bkyaLpRXK1SursEit7nU+m0YelyYzg\nGkAGh5Be1Sl9SU7Zzdc2Vb2UWXMaMdYkYBvv/t8NjtDXX/5YkcoiZbxsR59IKMyYo2yvrE9Jr0Fd\nS+NoddkwGRhGmqWT3jhRaGHZoU4V18t1GZWZt4c7v0ivdN2ehJT4+uwzXY/bNAqP2Gar20N2t2m/\nBFFlnqrOhbsh4AKlfRfq3QZ60a0MLjIHhXF4fvCjqVwi889T3Aspffj4s+dyzyHqMJ35MM/mI9v5\nen3bqe1nhDIKYqQghjGE/fz6+HZL3376qtbpy4fbu2o0u3J/tP/Jgd4++uu/+Pbh33xh+msf2Zfq\nm9zkDQNbqnLYL3Q5vf3iR+1y8mdxePRoBTYjHMYUujB4vP1J0nt/4cWLn/2Vf/mVH3z5R9/6xoel\n9eIaeaAx3vmlj1p79vf/GeuvdLqVb6KuEwtPtm10efnjj/ef0Ke/+e+e/uQvvXj54vSB66UG0aji\ngxZOuVz1yX635ncOm2/0rx/+/beOXxMoOGT/eHn8M1+7GRf7+i/9Z92su7vt5r8/+rN9kVJFZf3i\nW/0rP9/+6NE//9pay8/mp4cv3j/6pGoGKZdYvVzO773dlut7H73RJ3/j2bsP79XjZx9KIWJ/Itsn\nX9/oi89+7vv/8D/pnz57/dVpmr74ZAYXma/ls08//+UPdn+zft85PjqMO54+/tL+RLMs7q3xg/Vx\n7pEvDl5//ZtfPPnhzzx/Pj8ea99Ucvhhr+/f1ud+x8fDL+glHv3kw0O7+/aWiwpvj73+4sNP/s75\n5/7R09i99/r1gexmCSvsGet2PcRlPY13uJzK5fuPN+/evbZro0aj70QpB76WHx6+vX+sL+bv/qau\ng/ff/JVt+SVPgGp5iLbnH/6rv/ziZ46H8NiN96Jc11kcJoyLlulkiz2rtdf2IkRSv3SWZV48jSU1\nWzvcv3lHdPfy5V/RKzK/+vt/mw5rcILHwK5+IP5HiGku/cqFMyQkkfAALNAcp9dzq34qUm50u22n\nXe6DiIOJZDt/J9ZJN9f1uXK5bEw/fZ8grHldfad1+sZ58XD0dUL2taiIgLSDeWXZesm+5QSJUJmq\nT4OIoGqcelVap6cbGXPyx3qrh7L7hgWoEFsfUJ42u9uHsy2DN6P0BZPWohzO/QEGTfHb86W0NOG0\n6OwaSIYxMZKm//ELu1yJnnxy0qdbzU355gsVSs9IFWSOoDIgATeqMrMIig/2KwVA1flgV4gyOK4x\nR3N1cQkeoPL5dz/56ofa5IMfie5wGj/+ePnOr0lQMlMa53kdToZQYNCmUGNhzxAQryDeRMQkJszs\nKWRs4gwhSQoqa/nK8XtP99jLIxU7vjyOZ8vDTJmARIhJH+KgTZesrEhlikxJ5UpBxLqwonpIgRAh\nwYnUYCcKe+dP7javH6hAn+m8Lq/zqTxzRIA4OZFrQToTShehwoYUuAsnsnVYKR3EUi09qpOmQbMk\ns2ci5KN+uMZg31SGQfT987stggBXaCYGc2MaDq3CXFgzUFjhMYg1S3NuLE1DLMWhRAJQCrvD3/+i\nS1Htjx7UmUnf2tPPnRGIFqPQqMY2kvinzKgkIyl8qKaGM0g3nglhGBhMLIHgRCDS2Wy1ur29v/0T\nPd3GZonl5s3VhLjpaKRkpWfllUsWyg3FYsSrFaoERRDFxjHgAiHW5CRhI8mkTPSFh395r9dn39N1\nr/JnYznvFkcyNNSQSkqITXBwzlLePICqXXYKNvaUuGwLIiU1spAEJIjgNBCFHzZ/cfRnuL998X/4\n3c9vPvrq7Vd/d8vmaAIiDeKUwoAw0LSN+3XpJgcpRGkeptPaWFpn04klwQTPkZROGaNuHt3QNQ5/\n/JK39snY1A8PP0RxYSWGMRMplaqJKhA7DkthQGeQhGTkLgZvdBZlBJDIDGE37ek5PQrCm+Ptj3+/\n8H/8Mj5+2E2/+JaQSUwlE5UbSQ5iYYatRrW2FGEnH+qM8LKQUK1JIkQsGUQiEQju25I2xp5/7wvw\nv3j7bpN9Pfz8yp6mSaUyFSEe6QKCmyV012oTJxinCMzJLIgAdtKizFDJTFAKomb2ufzBH26Z7/7J\ntU4GkmBErBosXCGMLBwZQe4+TXWa26Y8emeKJA9EJwthnUQpqQiUiIJBaYHLNcbyu78jGVp+/I//\nwTv1NLyGBTKFrbhyQNoKhHNAeVNo4IB7701e9Um4D6ogJ4aEgDg5iC0ikbe19E++9fkbpeBy++k/\n/S9lI/NtuKdngItUCqgLexpl0OauHK9nfkJn0uW6nOeGESAQu3ApJFUERERBY1vK6Vv/4X9fagap\nXrfxb8ffuimPv7nzMXNGQ3ENhzhlwphOB/v+D5598Pzp5Pnienz547/b4u1H0MEkRBpAkgQ5fPAo\nn734wef34kSp2kuG/vYf/9ov35X0DIUABkOJ4EAws5z+17P13WdLeVtpROBpfu/Jp/smkGSEDGIE\nETlihNBvHeMSRtXYTSmX6vPzf/0H23d+VchDYTUoJQtFCjFrvPvpQ9uwnsHwo7Xx6Pr68KSCEmzV\nJRTlOl2JmFBefawhxhkUQjpQnEYZb99896+59ImgXpKQIQNUfVD6dlZwEYIfaS7IeaapJkCUJBL/\n36ZxWJRPli27IJLJlYWNU1KQ91+gy08HFQqJuFMS8yWMulFAt+xyU5hEsmQKicItQ0cQe1HKkNNn\n6smUHhkMFpdEgODb/1pcCERKkgKQsESQDHDagmFrBMN6JlVJygS4MUUQUY6ksJyfv5CSPSQ1EM7B\nmbCktd39H7gFAYBGBhgk5kaemcbp6RG6BAHMzj1BBFF1qkKDCeZDf+AU1DyILDRZkgTihMDnb3Mk\nkTBIQAS10WMEpE2zeHglkkKehzLo6iPDgpAamQFEUpbzJ9AkawtC4INd2XMwsXP9Dsk1MhAkhSg8\nsyRIyjyxRo7TJQJFoNKH9G6dxiBiBjrAOVyfvwkg1AleTUSRBC/WIu3mB3/ViykFgCQmF1MnZXB4\n+hJDuUnUoqx9WstSTQYixZCZFknyo2v1gHEZJUqQSk+SKJSk87e7gpJciJJNbbRoYgH4SCH3rj5F\nURfacNF0BK8lOZhg0RHrJ8KjDqohMWoxdiqaQc6y8vpDy0gi8kQmaK61qErVUgTbqUwyFwETqkhh\nERBdnTtSQI6Ynr/wSGZ2MtIwMJf0QuDhQptvliS27CAn5lpIUtrMfFlPY+kiuJyWy5lAhUUgzCQ9\nKcM9QUZfPDRuqxOPypEEDSYa1eqqgf233j7pvAtOSkKUFWcr5fr6SiO4rKYuAhvHOm+bdICJdThJ\nlz4wFjnDOdqi3rqwUygDDOfBkODtf/sNllGAYHK1vr59zXo5RV9OcpOZbV7RCVHr4y3HNG3yp23q\n6eCVB9TLMi+kRA5iJVNaqzGCmHe//euaNFpQisR6vP/81BuNNRPl5HalaQ1Yq5EvSp3vbn2flBkO\nJ5yvGwtiZxcakuBgtSJZACtpBJff+3XipJQg5Ah/52Y1E5draI9YqtNFV2mdSi1VaN2AMTiiU18d\nkEBIkDUTD00Ful63q1jJKDr/1q9WVCeBJFj3k4zz2UDNESMa315Xj2xDeZ50O3UjJbHsgesiAGUK\nkcnQXjJSNWQ0Z3UvEc79d/5eTStDiWyORo1vz8clm2cQ9R61uEkD5laazTPVNHe3yKszqlGDT5Qs\nkhRFUzuLl2QLHjqe/u6va0UgAc1DnIla2Z1HLKsnFfEI1TZTq0WjigQb5ejZ41pNKYUC7LNbvWwG\nKZKSczBC4V48Pm2TZImkdpXillZyY7L2kdbLyBAWzSrFlYBBSWEj4gGMokj06SwebDUZSpmyzANZ\nI9pw9fG6TDU44bWXSkHOIdTqmtGbsyPMpTCg6RGevlj6sjaSFFHXpEzqGvOpaBBaZ3BQyc5rmbg8\nJNNkakjT8DqkGDpgkaYoQ1KpphBHBFle3UZfsvAok9UooR5UkrskM6Q352QLNJpofVOWJYaZ09K7\nAxCSKhzJPc0ylUppERm0jNF79LTRr3MNJW+eIyNYTIMm4xAKHiIEwFmiwuty8WGDs2dmWjqxFk6w\nauT46ZlWnC4EDVxihK/SGKK8pgalmCAZDAVxejExaGceaUOHPiRySihAnBEISkKwiQcHrASJkQmc\nritZrOsNWwElIQWguoIN4ZpkmhEgOPUixFaRfMW0eJNOHMwZMZQok4wjQHqtVoI5IkaQ0+W0UREX\ntRBONXhqZ/KJQaAhkaJJKlFkZPE5j1eKMInhq6UHDbMe4bY6DYcVB5P1bjHG0nVbBUVMQcZeScgU\ngGsGxXyeHa4BTr5oyhyHl/dRbdPbopLhC2WM8PShPQenehnpERRXsxWTsm8j1TxhCJiyZwlSyGCr\nPKg65ms9hwKV/e7VgNNMxlkIvCaQMFLTESvCSTwR4zLclTaFFVirMNk8PHMy6CipnPDSObizGuE0\nahIV+NNXIZ1JuNegDS1MzOYERIyp8uAgt8uFvKI0rkwoYcji6fPZvXkZYHdJ51SmQG/HXmYX8CTT\nbiz9zQXZoBJ7DWfixpqZbaqYiC6+2hIVm8ZTIURoJqczejV1JRZ2GkLEI1MHr2fpOxoamLDbT76c\nehKhCpoHs7JSkTZVIo7e748rVW21Vm0SaTYcgCOYRZBWFMVcI5qD2cJXvgFBIXLZdH3oRsUKU+jO\nDBrQaFAMHnmih1E2hWvBzCaOYMUyWU0aasUhprIygV2CHWuC50LqSkAboLvjFQfpLQmVnaplwAPr\nEB/LpU/bJJ1mUYQF8eBlZnYqJjSma4Nau84GQQYvmTFCCOJSUstV7nfj3GsZNS2bOkh4jbGYnLFe\n/MAoJLUwGGIemmHgSC+hHkqmxVpX5+DAoisxXn2QoeRC6nNd9nU5s0ys3YHuPMjToo/urSGVapXi\n8EQmxYVzrV5gnGoI8KhJJGaMNYkIu29XkNEQoZn2M7XNO/t4OK1LjNVs2FjCl3W07RYKLlayUDKB\nIu0+qRd2IrgCgFZn4yC2+hpEhEd/+BviOuooVI12wxq09XFZyZPUY4Qbq5AYwNr3TZdEsLHXt280\ninmRzLYwJ/0/S7JsNfhWRLwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.PpmImagePlugin.PpmImageFile image mode=L size=92x112 at 0x7FF503FD7AC8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7daloQFSxagR",
        "colab_type": "code",
        "outputId": "d8f2064b-c542-430b-8796-1495086d4e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "img = read_image(dataset_path+'s1/1.pgm')\n",
        "img.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112, 92)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJ2boyzxeeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = 2\n",
        "total_sample_size = 10000\n",
        "\n",
        "\n",
        "def get_data(size, total_sample_size):\n",
        "    #read the image\n",
        "    image = read_image(dataset_path+ 's' + str(1) + '/' + str(1) + '.pgm', 'rw+')\n",
        "    #reduce the size\n",
        "    image = image[::size, ::size]\n",
        "    #get the new size\n",
        "    dim1 = image.shape[0]\n",
        "    dim2 = image.shape[1]\n",
        "\n",
        "    count = 0\n",
        "    \n",
        "    #initialize the numpy array with the shape of [total_sample, no_of_pairs, dim1, dim2]\n",
        "    x_geuine_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])  # 2 is for pairs\n",
        "    y_genuine = np.zeros([total_sample_size, 1])\n",
        "    \n",
        "    for i in range(40):\n",
        "        for j in range(int(total_sample_size/40)):\n",
        "            ind1 = 0\n",
        "            ind2 = 0\n",
        "            \n",
        "            #read images from same directory (genuine pair)\n",
        "            while ind1 == ind2:\n",
        "                ind1 = np.random.randint(10)\n",
        "                ind2 = np.random.randint(10)\n",
        "            \n",
        "            # read the two images\n",
        "            img1 = read_image(dataset_path+ 's' + str(i+1) + '/' + str(ind1 + 1) + '.pgm', 'rw+')\n",
        "            img2 = read_image(dataset_path+ 's' + str(i+1) + '/' + str(ind2 + 1) + '.pgm', 'rw+')\n",
        "            #reduce the size\n",
        "            img1 = img1[::size, ::size]\n",
        "            img2 = img2[::size, ::size]\n",
        "            \n",
        "            #store the images to the initialized numpy array\n",
        "            x_geuine_pair[count, 0, 0, :, :] = img1\n",
        "            x_geuine_pair[count, 1, 0, :, :] = img2\n",
        "            \n",
        "            #as we are drawing images from the same directory we assign label as 1. (genuine pair)\n",
        "            y_genuine[count] = 1\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    x_imposite_pair = np.zeros([total_sample_size, 2, 1, dim1, dim2])\n",
        "    y_imposite = np.zeros([total_sample_size, 1])\n",
        "    \n",
        "    for i in range(int(total_sample_size/10)):\n",
        "        for j in range(10):\n",
        "            \n",
        "            #read images from different directory (imposite pair)\n",
        "            while True:\n",
        "                ind1 = np.random.randint(40)\n",
        "                ind2 = np.random.randint(40)\n",
        "                if ind1 != ind2:\n",
        "                    break\n",
        "                    \n",
        "            img1 = read_image(dataset_path + 's' + str(ind1+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
        "            img2 = read_image(dataset_path + 's'+ str(ind2+1) + '/' + str(j + 1) + '.pgm', 'rw+')\n",
        "\n",
        "            img1 = img1[::size, ::size]\n",
        "            img2 = img2[::size, ::size]\n",
        "\n",
        "            x_imposite_pair[count, 0, 0, :, :] = img1\n",
        "            x_imposite_pair[count, 1, 0, :, :] = img2\n",
        "            #as we are drawing images from the different directory we assign label as 0. (imposite pair)\n",
        "            y_imposite[count] = 0\n",
        "            count += 1\n",
        "            \n",
        "    #now, concatenate, genuine pairs and imposite pair to get the whole data\n",
        "    X = np.concatenate([x_geuine_pair, x_imposite_pair], axis=0)/255\n",
        "    Y = np.concatenate([y_genuine, y_imposite], axis=0)\n",
        "\n",
        "    return X, Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCzNb4FrxoZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = get_data(size, total_sample_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "121fpwl9ycIy",
        "colab_type": "code",
        "outputId": "dae6ad85-4c30-45ec-c166-9074d492e1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#there is 20000 rows .....in 1 row ,2 images are there of 56 by 46 ..and is B/W image\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 2, 1, 56, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tm5CsOU1UZ0",
        "colab_type": "code",
        "outputId": "dc04159c-5d4e-481e-98f9-9c841dda6086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtVVfkYy1lrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIRRQ7bv2WpU",
        "colab_type": "code",
        "outputId": "cb621fde-1601-463c-b406-47dc96863207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsZt_Juv2ZTc",
        "colab_type": "code",
        "outputId": "f415140d-9667-44d1-a9e2-e026e7a9fc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRrCrM9W2k6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this will act as a base layer\n",
        "\n",
        "def build_base_network(input_shape):\n",
        "    \n",
        "    seq = Sequential()\n",
        "    \n",
        "    nb_filter = [6, 12]\n",
        "    kernel_size = 3\n",
        "    \n",
        "    \n",
        "    #convolutional layer 1\n",
        "    seq.add(Convolution2D(nb_filter[0], kernel_size, kernel_size, input_shape=input_shape,\n",
        "                          border_mode='valid', dim_ordering='th'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "    seq.add(Dropout(.25))\n",
        "    \n",
        "    #convolutional layer 2\n",
        "    seq.add(Convolution2D(nb_filter[1], kernel_size, kernel_size, border_mode='valid', dim_ordering='th'))\n",
        "    seq.add(Activation('relu'))\n",
        "    seq.add(MaxPooling2D(pool_size=(2, 2), dim_ordering='th')) \n",
        "    seq.add(Dropout(.25))\n",
        "\n",
        "    #flatten \n",
        "    seq.add(Flatten())\n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(50, activation='relu'))\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhoAV5N92v2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "input_dim = x_train.shape[2:]\n",
        "# Input(shape=input_dim) returns Tensor\n",
        "# understand what tenor is https://towardsdatascience.com/quick-ml-concepts-tensors-eb1330d7760f\n",
        "# understand about tensor from docs https://keras.io/getting-started/functional-api-guide/\n",
        "#img_a and img_b are tensor objects and it is input layer\n",
        "# Unlike the Sequential model, you must create and define a standalone Input layer that specifies the shape of input data.\n",
        "# The input layer takes a shape argument that is a tuple that indicates the dimensionality of the input data.\n",
        "\n",
        "img_a = Input(shape=input_dim) #input layer\n",
        "img_b = Input(shape=input_dim) #input layer\n",
        "#output of firt layer i.e img_a and img_b will be given to the input of next level which is whole 1 layer i.e base_network layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2eC91c623R1",
        "colab_type": "code",
        "outputId": "909fe75c-d07a-4bff-dfbb-2564b2b171a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "img_a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'input_2:0' shape=(?, 1, 56, 46) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjVoFD3Q_rpJ",
        "colab_type": "code",
        "outputId": "7ec9892f-cd63-4e48-d89d-128ef76acff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "img_a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(1), Dimension(56), Dimension(46)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN8ymLJgACFX",
        "colab_type": "code",
        "outputId": "0da02d90-f252-449c-9955-1ba2944640bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "# for more docs--->https://keras.io/getting-started/functional-api-guide/\n",
        "# All models are callable, just like layers...in build_base_network(), seq is model and base_network will be = seq after function call...therefoe base_network is copy of seq hence it is also model...further there is base_network(img_a)....therefore we are caling the model and passing the tensor object to model\n",
        "\n",
        "# for more https://keras.io/getting-started/functional-api-guide/\n",
        "# base_network = seq..... which will return from function\n",
        "# from docs----> (With the functional API, it is easy to reuse trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just reusing the architecture of the model, you are also reusing its weights.)\n",
        "# hence base_network is the model itself\n",
        "# tutorial--->https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
        "\n",
        "#I thik we are using functional API bcoz we arenot using CNN to train our dataset...we just want feature of images Using CNN ...therefore we have to use functional API to get intermediate layer outputs...\n",
        "#we are using functional API bcoz we want 2 CNN at the same time for 2 images...therefore it is ot possible using directly CNN...we have functional api which is used to make complex models... hence we are using it\n",
        "base_network = build_base_network(input_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(6, (3, 3), input_shape=(1, 56, 46..., padding=\"valid\", data_format=\"channels_first\")`\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (3, 3), padding=\"valid\", data_format=\"channels_first\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FmmhkhQALAi",
        "colab_type": "code",
        "outputId": "a0080e6e-dc61-4d62-8d7a-89f20b64d366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "base_network"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7ff4fdf32080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMzcgqsFAT3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The layers in the model are connected pairwise.\n",
        "# his is done by specifying where the input comes from when defining each new layer. A bracket notation is used, such that after the layer is created, the layer from which the input to the current layer comes from is specified.\n",
        "# It is this way of connecting layers piece by piece that gives the functional API its flexibility\n",
        "#consider base_network as the 1 whole layer itself...input img is given to this whole first layer..'feat_vecs_a' is like 'output_1' in bottommost commented code\n",
        "#here we wantto CNN at the same time therefore fea_vecs_b is also there\n",
        "#we can see shape of feat_vecs_a is (?,50) bcoz at last layer in function base_network() is seq.add(Dense(50, activation='relu'))...therefore it will have 50 nodes\n",
        "#{\n",
        "#inputs = Input(shape=(784,))\n",
        "#output_1 = Dense(64, activation='relu')(inputs)\n",
        "#}\n",
        "#compare above code to below\n",
        "#{\n",
        "#img_a = Input(shape=input_dim)<---input layer\n",
        "#feat_vecs_a = base_network(img_a) \n",
        "#}\n",
        "#in above first example output_1 would have shape (?, 64)\n",
        "#in above second example we have 50 nodes in last layer in base_network therfore shape of feat_vecs_a is(?,50)\n",
        "feat_vecs_a = base_network(img_a)\n",
        "feat_vecs_b = base_network(img_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNmc6yIhEd-J",
        "colab_type": "code",
        "outputId": "641ad686-e9fe-4a82-fe6c-72dd750e76d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feat_vecs_a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'sequential_1/dense_3/Relu:0' shape=(?, 50) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3mb8nORFWIz",
        "colab_type": "code",
        "outputId": "773cb231-f8b1-4cab-815a-df4f4e14af75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feat_vecs_a.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([Dimension(None), Dimension(50)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuFTq0IYF12y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tUAaND1R-AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we are here MANUALLY applying the classification algorithm to find distance beteen 2 images...this logic usually written in algorithm itself when we apply sigmoid or other clssification function in standard CNN method...but we ae using fcuntional apis ad we have 2 cnn as the same time so we have to write our logic seperately....\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([feat_vecs_a, feat_vecs_b])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We_RaOzRSD0y",
        "colab_type": "code",
        "outputId": "dd1768d5-59b5-475c-d89a-6f30ad7dfdb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#here distance is only 1 tensor i.e 1 node(confirm node and tensors are same or diffrent..?) which we usually see at the end of CNN...which will be the integer number which will denote the distnce between 2 images\n",
        "distance"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'lambda_1/Sqrt:0' shape=(?, 1) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t6k53V2SFeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 13\n",
        "rms = RMSprop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_QRgZcqTw2y",
        "colab_type": "code",
        "outputId": "db95741b-aa69-4900-8648-7129971409f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#Keras provides a Model class that you can use to create a model from your created layers. It requires that you only specify the input and output layers.\n",
        "#img_a and img_b are 2 input layers and distance is output layer\n",
        "model = Model(input=[img_a, img_b], output=distance)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"la...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0grlzTqT0nT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocfAr1qlUFNq",
        "colab_type": "code",
        "outputId": "f0d0259f-9449-4401-856a-6dbccd2a7b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "model.compile(loss=contrastive_loss, optimizer=rms)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6IzxG7cUJFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#img_1 and img2 will contain all the 15000 train images\n",
        "img_1 = x_train[:, 0]\n",
        "img2 = x_train[:, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZomQvgBYUUDJ",
        "colab_type": "code",
        "outputId": "9b36e3ed-6122-47bb-8c50-b4b8d7803bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "img_1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 1, 56, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaxYP2JpUV7N",
        "colab_type": "code",
        "outputId": "c323f620-8c6f-4eb9-ce50-c7dc6d26baca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "img2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 1, 56, 46)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECwmiyzQUiHW",
        "colab_type": "code",
        "outputId": "c2d4a4aa-01e9-4f97-81ce-6064c83125e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "model.fit([img_1, img2], y_train, validation_split=.25,\n",
        "          batch_size=128, verbose=2, nb_epoch=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 11250 samples, validate on 3750 samples\n",
            "Epoch 1/13\n",
            " - 8s - loss: 0.1692 - val_loss: 0.1935\n",
            "Epoch 2/13\n",
            " - 1s - loss: 0.1004 - val_loss: 0.1249\n",
            "Epoch 3/13\n",
            " - 1s - loss: 0.0757 - val_loss: 0.1081\n",
            "Epoch 4/13\n",
            " - 1s - loss: 0.0613 - val_loss: 0.0866\n",
            "Epoch 5/13\n",
            " - 1s - loss: 0.0503 - val_loss: 0.0625\n",
            "Epoch 6/13\n",
            " - 1s - loss: 0.0445 - val_loss: 0.0542\n",
            "Epoch 7/13\n",
            " - 1s - loss: 0.0397 - val_loss: 0.0452\n",
            "Epoch 8/13\n",
            " - 1s - loss: 0.0366 - val_loss: 0.0385\n",
            "Epoch 9/13\n",
            " - 1s - loss: 0.0340 - val_loss: 0.0302\n",
            "Epoch 10/13\n",
            " - 1s - loss: 0.0317 - val_loss: 0.0305\n",
            "Epoch 11/13\n",
            " - 1s - loss: 0.0291 - val_loss: 0.0237\n",
            "Epoch 12/13\n",
            " - 1s - loss: 0.0277 - val_loss: 0.0203\n",
            "Epoch 13/13\n",
            " - 1s - loss: 0.0257 - val_loss: 0.0201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff48cbaeeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkNeHZITU9uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict([x_test[:, 0], x_test[:, 1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0pKHnyaVQ_5",
        "colab_type": "code",
        "outputId": "0d5c6ca2-83cb-4daf-87f2-bea0097f2eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTYvJR4KVSEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(predictions, labels):\n",
        "    return labels[predictions.ravel() < 0.5].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4GXCJDFVah6",
        "colab_type": "code",
        "outputId": "b08aafe6-c4fb-449a-bb83-5bd9ababa13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "compute_accuracy(pred, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9680972274971515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddgv48RbWSv9",
        "colab_type": "code",
        "outputId": "79e87099-ffa0-44b7-98d3-35d2e485f430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test.ravel().astype(int)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9c5FgnIW2Bg",
        "colab_type": "code",
        "outputId": "b5cecf78-584b-4a35-b4eb-c7a74f7548f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(pred.ravel()<0.5).astype(int)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2esdzge6VxZA",
        "colab_type": "code",
        "outputId": "6878eacc-d685-4664-b47e-c3c15ca8307e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " len(y_test[pred.ravel()<0.5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2633"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd-Ll1bfdazj",
        "colab_type": "code",
        "outputId": "b98c7d07-7b1d-4df4-b365-a77af9c72514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        " y_test[pred.ravel()<0.5]\n",
        "  "
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       ...,\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHdGVm2riDSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c6193fd8-48dd-4df9-8319-a9cf36bcecd1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n",
        "confusion_matrix(y_test.ravel().astype(int),(pred.ravel()<0.5).astype(int))\n",
        "\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2367,   84],\n",
              "       [   0, 2549]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EPnlN9Ki4yu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5cae1ae-a083-47fa-f843-07a97db48304"
      },
      "source": [
        "from sklearn.metrics import accuracy_score \n",
        "accuracy_score (y_test.ravel().astype(int),(pred.ravel()<0.5).astype(int))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIJJZ8ETBsh0",
        "colab_type": "text"
      },
      "source": [
        "#Always open these 2 links and try to understand above code....its fucking awesome\n",
        "\n",
        "*  https://keras.io/getting-started/functional-api-guide/\n",
        "* https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
        "* https://github.com/sudharsan13296/Hands-On-Meta-Learning-With-Python/blob/master/02.%20Face%20and%20Audio%20Recognition%20using%20Siamese%20Networks/2.4%20Face%20Recognition%20Using%20Siamese%20Network.ipynb\n",
        "* take a look at this....https://blog.eduonix.com/artificial-intelligence/functional-api-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOVj7wOG_X7-",
        "colab_type": "text"
      },
      "source": [
        "* In functional API Models are defined by creating instances of layers and connecting them directly to each other in pairs, \n",
        "then defining a Model that specifies the layers to act as the input and output to the model.\n",
        "###compare following code with above our code\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USourDqCAWa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uncomment and then take look for better understandig\n",
        "'''\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=(784,))     #input layer\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "output_1 = Dense(64, activation='relu')(inputs)\n",
        "output_2 = Dense(64, activation='relu')(output_1)\n",
        "predictions = Dense(10, activation='softmax')(output_2)   #output layer\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(data, labels)  # starts training\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXvJmjzqAXQm",
        "colab_type": "text"
      },
      "source": [
        "* Input(shape=(784,)) returns tensor\n",
        "* now we are step by step making layers in above code similarly    as we do with Sequencial model\n",
        "* output_1 = Dense(64, activation='relu')(inputs) ..here 'inputs' tensor we have given to Dense layer as input...output of this layer will be given to next layer..likewise will keep giving ones outputs to other layer inputs,...\n",
        "* 'predictions' is the final output layer \n",
        "* Keras provides a Model class that you can use to create a model from your created layers. It requires that you only specify the input and output layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VArjTgs9AEDr",
        "colab_type": "code",
        "outputId": "5c767794-beea-4548-c4e2-71511b43238a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "inputs = Input(shape=(784,))\n",
        "output_1 = Dense(64, activation='relu')(inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UMje8_WLo4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}